{% extends "base.html" %}

{% block content %}
<div class="container">
  <h2>ğŸ¤ Voice AI Assistant</h2>
  
  <!-- Microphone animation circle -->
  <div id="mic-visual"></div>

  <div class="btn-group">
    <button id="speakBtn">ğŸ™ï¸ Start Listening</button>
    <button id="togglePauseBtn">â¯ Pause / Resume</button>
    <button id="cancelBtn">âŒ Cancel Speech</button>
  </div>

  <p id="status">Ready to assist you ğŸ¤–</p>
  <div id="response"></div>
</div>

<script>
  const speakBtn = document.getElementById("speakBtn");
  const togglePauseBtn = document.getElementById("togglePauseBtn");
  const cancelBtn = document.getElementById("cancelBtn");
  const status = document.getElementById("status");
  const responseBox = document.getElementById("response");
  const micVisual = document.getElementById("mic-visual");

  let recognition;
  let currentUtterance = null;

  if (!('webkitSpeechRecognition' in window)) {
    status.innerText = "âŒ Speech recognition not supported. Use Chrome.";
  } else {
    recognition = new webkitSpeechRecognition();
    recognition.lang = "en-US";
    recognition.continuous = false;
    recognition.interimResults = false;

    recognition.onstart = () => {
      status.innerText = "ğŸ¤ Listening...";
      micVisual.classList.add("listening");
    };

    recognition.onerror = (event) => {
      status.innerText = "âš ï¸ Error: " + event.error;
    };

    recognition.onend = () => {
      status.innerText = "âœ… Stopped listening.";
      micVisual.classList.remove("listening");
    };

    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript;
      status.innerText = "ğŸ“ You said: " + transcript;

      fetch("/voice", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text: transcript })
      })
      .then(res => res.json())
      .then(data => {
        responseBox.innerText = "ğŸ¤– AI: " + data.response;

        // âœ… Speak AI response
        currentUtterance = new SpeechSynthesisUtterance(data.response);
        window.speechSynthesis.cancel(); // stop any ongoing speech
        window.speechSynthesis.speak(currentUtterance);
      })
      .catch(err => console.error("Fetch error:", err));
    };

    // ğŸ¤ Start listening
    speakBtn.onclick = () => {
      recognition.start();
    };

    // â¯ Toggle pause/resume
    togglePauseBtn.onclick = () => {
      if (speechSynthesis.speaking) {
        if (speechSynthesis.paused) {
          speechSynthesis.resume();
          status.innerText = "â–¶ Resumed speech.";
        } else {
          speechSynthesis.pause();
          status.innerText = "â¸ Paused speech.";
        }
      }
    };

    // âŒ Cancel AI speech completely
    cancelBtn.onclick = () => {
      speechSynthesis.cancel();
      status.innerText = "âŒ Speech canceled.";
    };
  }
</script>
{% endblock %}
